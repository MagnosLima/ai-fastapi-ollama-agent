# FastAPI + Ollama Agent API

API de chat desenvolvida com **FastAPI** integrada a um **Agente de IA utilizando Strands Agents SDK** e **Ollama** como provedor LLM local.  
O projeto inclui uma **tool de cÃ¡lculo matemÃ¡tico** que o agente utiliza automaticamente quando identifica operaÃ§Ãµes numÃ©ricas na pergunta.

---

## âœ… Funcionalidades

- âœ… Endpoint `POST /chat`
- âœ… IntegraÃ§Ã£o direta com Ollama via `Strands Agents`
- âœ… Tool de cÃ¡lculo matemÃ¡tico automÃ¡tica
- âœ… Respostas em JSON
- âœ… ExecuÃ§Ã£o local
- âœ… ConfiguraÃ§Ã£o por `.env`
- âœ… Estrutura organizada por mÃ³dulos

---

## ğŸ§  Arquitetura

- **FastAPI** â†’ API HTTP
- **Strands Agents SDK** â†’ OrquestraÃ§Ã£o do agente
- **Ollama (local)** â†’ LLM
- **Tool Calculator** â†’ CÃ¡lculos matemÃ¡ticos automÃ¡ticos

ComunicaÃ§Ã£o com o Ollama Ã© feita **diretamente pelo Strands**, sem uso de `httpx`.

---

## ğŸ“ Estrutura do Projeto

    dreamsquad-ia-chat/
    â”œâ”€â”€ app/
    â”‚   â”œâ”€â”€ agent/
    â”‚   â”‚   â”œâ”€â”€ agent.py
    â”‚   â”‚   â””â”€â”€ tools/
    â”‚   â”‚       â””â”€â”€ calculator.py
    â”‚   â”œâ”€â”€ core/
    â”‚   â”‚   â””â”€â”€ config.py
    â”‚   â”œâ”€â”€ schemas/
    â”‚   â”‚   â””â”€â”€ chat.py
    â”‚   â””â”€â”€ main.py
    â”œâ”€â”€ venv/
    â”œâ”€â”€ .env
    â”œâ”€â”€ .env.example
    â”œâ”€â”€ .gitignore
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ README.md

------------------------------------------------------------------------

## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente

### 1ï¸âƒ£ Clonar o repositÃ³rio

``` bash
git clone https://github.com/MagnosLima/ai-fastapi-ollama-agent.git
cd ai-fastapi-ollama-agent
```

------------------------------------------------------------------------

### 2ï¸âƒ£ Criar e ativar ambiente virtual

``` bash
python -m venv venv
```

#### Windows (PowerShell / Git Bash):

``` bash
venv\Scripts\activate
```

#### Linux / Mac:

``` bash
source venv/bin/activate
```

#### Git Bash:

``` bash
source venv/Scripts/activate
```

------------------------------------------------------------------------

### 3ï¸âƒ£ Instalar dependÃªncias

``` bash
pip install -r requirements.txt
```

------------------------------------------------------------------------

### 4ï¸âƒ£ Instalar e configurar o Ollama

âœ… Baixar o Ollama: https://ollama.com

âœ… Baixar o modelo utilizado no projeto:

``` bash
ollama pull qwen3:4b
```

âœ… Verificar se o Ollama estÃ¡ rodando:

``` bash
ollama list
```

âš ï¸ O servidor padrÃ£o do Ollama roda em:

    http://localhost:11434

------------------------------------------------------------------------

### 5ï¸âƒ£ Configurar variÃ¡veis de ambiente

Crie um arquivo `.env` baseado no `.env.example`:

``` env
LLM_MODEL=qwen3:4b
LLM_ENDPOINT=http://localhost:11434
```

------------------------------------------------------------------------

## â–¶ï¸ Executando a AplicaÃ§Ã£o

Com o ambiente virtual ativo:

``` bash
uvicorn app.main:app --reload
```

âœ… A aplicaÃ§Ã£o estarÃ¡ disponÃ­vel em:

    http://127.0.0.1:8000

âœ… DocumentaÃ§Ã£o interativa (Swagger):

    http://127.0.0.1:8000/docs

------------------------------------------------------------------------

## ğŸ” Endpoint de Chat

### ğŸ“Œ POST /chat

### Exemplo de Request:

``` json
{
  "message": "Qual Ã© a capital da FranÃ§a?"
}
```

### Exemplo de Response:

``` json
{
  "response": "A capital da FranÃ§a Ã© Paris."
}
```

------------------------------------------------------------------------

## ğŸ§® Teste da Tool de CÃ¡lculo

Exemplos de perguntas:

``` json
{
  "message": "1234 * 5678"
}
```

Resposta esperada:

``` json
{
  "response": "7006652"
}
```

Outro exemplo:

``` json
{
  "message": "Qual a raiz quadrada de 144?"
}
```

``` json
{
  "response": "12"
}
```

------------------------------------------------------------------------

## ğŸ§  Funcionamento do Agente

-   O agente utiliza o modelo local do Ollama.
-   Caso identifique uma operaÃ§Ã£o matemÃ¡tica, ele chama automaticamente
    a **tool calculator**.
-   Caso contrÃ¡rio, responde utilizando o LLM normalmente.

------------------------------------------------------------------------

## ğŸ›‘ Encerrando a AplicaÃ§Ã£o

No terminal onde o Uvicorn estÃ¡ rodando:

    CTRL + C

------------------------------------------------------------------------

## ğŸ“¦ Versionamento

O projeto jÃ¡ possui `.gitignore` configurado para ignorar:

-   Ambiente virtual (`venv/`)
-   Arquivo `.env`
-   Cache do Python
-   Arquivos temporÃ¡rios
-   BinÃ¡rios do Ollama

------------------------------------------------------------------------

## âœ… Status do Projeto

âœ” API operacional\
âœ” IntegraÃ§Ã£o com Ollama funcionando\
âœ” Tool de cÃ¡lculo validada\
âœ” Swagger funcionando

------------------------------------------------------------------------

## ğŸ‘¨â€ğŸ’» Autor

[Magnos Lima ](https://github.com/MagnosLima)
